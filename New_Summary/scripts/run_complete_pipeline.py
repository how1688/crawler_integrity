"""
å®Œæ•´æ–°èè™•ç†æµæ°´ç·š
å°‡è³‡æ–™è™•ç†å’Œå ±å°ç”Ÿæˆä¸²è¯åŸ·è¡Œï¼Œç›´æ¥ç”¢ç”Ÿæœ€çµ‚çµæœ
"""

import os
import sys
import json
from datetime import datetime
from typing import List
import logging

# ç¢ºä¿è¼‰å…¥ .env æª”æ¡ˆ
try:
    from dotenv import load_dotenv
    load_dotenv(os.path.join(os.path.dirname(__file__), "../.env"))
except ImportError:
    pass

# æ·»åŠ çˆ¶ç›®éŒ„åˆ° Python è·¯å¾‘ï¼Œä»¥ä¾¿å¼•ç”¨ core æ¨¡çµ„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.news_processor import NewsProcessor
from core.config import NewsProcessorConfig
from core.report_generator import ReportGenerator
from core.report_config import ReportGeneratorConfig
from core.db_client import SupabaseClient
from core.difficult_keyword_extractor_final import DiffKeywordProcessor, DiffKeywordConfig

# è¨­ç½®æ—¥èªŒ - ç‚ºä¸åŒæ¨¡çµ„è¨­ç½®ä¸åŒçš„æ—¥èªŒæª”æ¡ˆ
def setup_logging():
    """è¨­ç½®å¤šå€‹æ—¥èªŒæª”æ¡ˆçš„æ—¥èªŒç³»çµ±"""
    # ç¢ºä¿æ—¥èªŒç›®éŒ„å­˜åœ¨
    os.makedirs('outputs/logs', exist_ok=True)
    
    # è¨­ç½®æ ¹æ—¥èªŒå™¨
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    
    # æ¸…é™¤ç¾æœ‰çš„ handlers
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # è¨­ç½®æ ¼å¼åŒ–å™¨
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    
    # è¨­ç½®æ§åˆ¶å°è¼¸å‡º
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)
    
    # ç‚ºä¸»ç¨‹å¼è¨­ç½®æ—¥èªŒæª”æ¡ˆ
    main_handler = logging.FileHandler('outputs/logs/complete_pipeline.log', encoding='utf-8')
    main_handler.setFormatter(formatter)
    
    # ç‚ºä¸åŒæ¨¡çµ„è¨­ç½®ä¸åŒçš„æ—¥èªŒæª”æ¡ˆ
    module_handlers = {
        'core.db_client': 'outputs/logs/db_client.log',
        'core.news_processor': 'outputs/logs/news_processing.log',
        'core.report_generator': 'outputs/logs/report_generation.log',
        'core.difficult_keyword_extractor_final': 'outputs/logs/keyword_extraction.log',
        'scripts.run_complete_pipeline': 'outputs/logs/complete_pipeline.log'
    }
    
    # ç‚ºæ¯å€‹æ¨¡çµ„å‰µå»ºå°ˆå±¬çš„ handler
    for module_name, log_file in module_handlers.items():
        module_logger = logging.getLogger(module_name)
        
        # é˜²æ­¢æ—¥èªŒé‡è¤‡åˆ°çˆ¶æ—¥èªŒå™¨
        module_logger.propagate = False
        
        # å‰µå»ºæª”æ¡ˆ handler
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(formatter)
        
        # æ·»åŠ æ§åˆ¶å°å’Œæª”æ¡ˆ handler
        module_logger.addHandler(console_handler)
        module_logger.addHandler(file_handler)
        module_logger.setLevel(logging.INFO)

# åˆå§‹åŒ–æ—¥èªŒç³»çµ±
setup_logging()
logger = logging.getLogger(__name__)

class CompletePipeline:
    """å®Œæ•´çš„æ–°èè™•ç†æµæ°´ç·š"""
    
    def __init__(self, api_key: str = None):
        """åˆå§‹åŒ–æµæ°´ç·š"""
        self.api_key = api_key or NewsProcessorConfig.get_gemini_api_key()
        if not self.api_key:
            raise ValueError("æœªè¨­å®š GEMINI_API_KEY")
        
        logger.info("ğŸš€ åˆå§‹åŒ–å®Œæ•´æ–°èè™•ç†æµæ°´ç·š")
        
    def run_complete_pipeline(self):
        """
        åŸ·è¡Œå®Œæ•´æµæ°´ç·š
        """
        
        start_time = datetime.now()
        logger.info(f"â° æµæ°´ç·šé–‹å§‹æ™‚é–“: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šæ–°èè³‡æ–™è™•ç†
            logger.info("\n" + "="*60)
            logger.info("ğŸ”„ ç¬¬ä¸€æ­¥ï¼šåŸ·è¡Œæ–°èè³‡æ–™è™•ç†")
            logger.info("="*60)
            
            processed_result = self._run_news_processing()

            if processed_result is None:
                logger.error("âŒ æ–°èè™•ç†å¤±æ•—ï¼Œæµæ°´ç·šçµ‚æ­¢")
                return None
            
            if not processed_result:  # ç©ºåˆ—è¡¨è¡¨ç¤ºæ²’æœ‰æ–°è³‡æ–™éœ€è¦è™•ç†
                logger.info("âœ¨ æ²’æœ‰æ–°è³‡æ–™éœ€è¦è™•ç†ï¼Œæµæ°´ç·šæ­£å¸¸çµæŸ")
                return []

            logger.info(f"âœ… æ–°èè™•ç†å®Œæˆï¼Œè™•ç†äº† {len(processed_result)} å€‹ stories")

            # ç¬¬äºŒæ­¥ï¼šå ±å°ç”Ÿæˆ
            logger.info("\n" + "="*60)
            logger.info("ğŸ“ ç¬¬äºŒæ­¥ï¼šåŸ·è¡Œå ±å°ç”Ÿæˆ")
            logger.info("="*60)

            report_result = self._run_report_generation(processed_result)

            if not report_result:
                logger.error("âŒ å ±å°ç”Ÿæˆå¤±æ•—ï¼Œæµæ°´ç·šçµ‚æ­¢")
                return None

            logger.info(f"âœ… å ±å°ç”Ÿæˆå®Œæˆï¼Œç”Ÿæˆäº† {len(report_result)} å€‹å ±å‘Š")

            # ç¬¬ä¸‰æ­¥ï¼šå„²å­˜åˆ°è³‡æ–™åº«
            logger.info("\n" + "="*60)
            logger.info("ğŸ’¾ ç¬¬ä¸‰æ­¥ï¼šå„²å­˜æ‘˜è¦åˆ°è³‡æ–™åº«")
            logger.info("="*60)
                      
            db_client = SupabaseClient()
            success_count = 0
            for idx, single_report in enumerate(report_result):
                story_id = single_report.get('story_info', {}).get('story_id', '')
                
                update_data = {
                    'story_id': story_id,
                    'category': single_report.get('story_info', {}).get('category', ''),
                    'total_articles': single_report.get('story_info', {}).get('total_articles', 0),
                    'news_title': single_report.get('comprehensive_report', {}).get('title', ''),
                    'ultra_short': single_report.get('comprehensive_report', {}).get('versions', {}).get('ultra_short', ''),
                    'short': single_report.get('comprehensive_report', {}).get('versions', {}).get('short', ''),
                    'long': single_report.get('comprehensive_report', {}).get('versions', {}).get('long', ''),
                    'generated_date': single_report.get('processed_at', '')
                }
                
                if db_client.save_to_single_news(story_id, update_data):
                    success_count += 1
                    logger.info(f"âœ… å„²å­˜æˆåŠŸ: {story_id}")
                else:
                    logger.error(f"âŒ å„²å­˜å¤±æ•—: {story_id}")
            
            logger.info(f"ğŸ’¾ è³‡æ–™åº«å„²å­˜å®Œæˆï¼š{success_count}/{len(report_result)} æˆåŠŸ")
            
            # ç¬¬å››æ­¥ï¼šç”Ÿæˆå›°é›£é—œéµå­—
            logger.info("\n" + "="*60)
            logger.info("ğŸ”¤ ç¬¬å››æ­¥ï¼šç”Ÿæˆå›°é›£é—œéµå­—")
            logger.info("="*60)
            
            # ç²å–éœ€è¦ç”Ÿæˆ terms çš„ story_ids
            updated_story_ids = db_client.get_updated_story_ids()
            
            if updated_story_ids:
                logger.info(f"ğŸ“ éœ€è¦ç”Ÿæˆ terms çš„ stories: {len(updated_story_ids)} å€‹")
                terms_success = self._run_keyword_extraction(list(updated_story_ids))
                
                if terms_success:
                    logger.info(f"âœ… å›°é›£é—œéµå­—ç”Ÿæˆå®Œæˆ: {len(updated_story_ids)} å€‹ stories")
                else:
                    logger.warning("âš ï¸ å›°é›£é—œéµå­—ç”Ÿæˆéƒ¨åˆ†å¤±æ•—ï¼Œä½†ä¸å½±éŸ¿ä¸»æµç¨‹")
            else:
                logger.info("âœ¨ æ²’æœ‰éœ€è¦ç”Ÿæˆå›°é›£é—œéµå­—çš„ stories")
            
            # æ¸…ç©ºæ›´æ–°è¨˜éŒ„
            db_client.clear_updated_story_ids()
            
            # çµæŸ
            end_time = datetime.now()
            duration = end_time - start_time
            logger.info(f"\nğŸ‰ æµæ°´ç·šåŸ·è¡Œå®Œæˆï¼")
            logger.info(f"â° ç¸½è€—æ™‚: {duration}")
            logger.info(f"ğŸ“Š è™•ç†çµæœ: {len(processed_result)} stories â†’ {len(report_result)} reports â†’ {success_count} saved")
            if updated_story_ids:
                logger.info(f"ğŸ”¤ å›°é›£é—œéµå­—: {len(updated_story_ids)} stories")
            
            return report_result

        except Exception as e:
            logger.error(f"âŒ æµæ°´ç·šåŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return None
    
    def _run_news_processing(self):
        """åŸ·è¡Œæ–°èè³‡æ–™è™•ç†"""
        try:
            
            # åˆå§‹åŒ–æ–°èè™•ç†å™¨
            processor = NewsProcessor(
                api_key=self.api_key, 
                model_name=NewsProcessorConfig.GEMINI_MODEL
            )
            
            # åŸ·è¡Œè™•ç†
            processor_result = processor.process_all_stories()
            return processor_result

        except Exception as e:
            logger.error(f"âŒ æ–°èè™•ç†å¤±æ•—ï¼š{e}")
            return None
    
    def _run_report_generation(self, processed_result):
        """åŸ·è¡Œå ±å°ç”Ÿæˆ"""
        try:
            # åˆå§‹åŒ–å ±å°ç”Ÿæˆå™¨
            generator = ReportGenerator(
                api_key=self.api_key,
                model_name=ReportGeneratorConfig.GEMINI_MODEL
            )
            
            
            # åŸ·è¡Œå ±å°ç”Ÿæˆï¼ˆåªç”Ÿæˆç¶œåˆå ±å°ï¼‰
            generator_result = generator.generate_reports_for_all_stories(processed_result)
            return generator_result
            
        except Exception as e:
            logger.error(f"âŒ å ±å°ç”Ÿæˆå¤±æ•—ï¼š{e}")
            return None
    
    def _run_keyword_extraction(self, updated_story_ids: List[str]) -> bool:
        """åŸ·è¡Œå›°é›£é—œéµå­—æå–"""
        try:
            logger.info(f"ğŸ”¤ é–‹å§‹ç‚º {len(updated_story_ids)} å€‹ stories ç”Ÿæˆå›°é›£é—œéµå­—...")
            
            # åˆå§‹åŒ–å›°é›£é—œéµå­—æå–å™¨
            keyword_processor = DiffKeywordProcessor()
            
            if not keyword_processor.is_ready():
                logger.warning("å›°é›£é—œéµå­—æå–å™¨æœªæº–å‚™å°±ç·’")
                return False
            keyword_processor.run(limit=None, story_ids=updated_story_ids)
            
            logger.info("âœ… å›°é›£é—œéµå­—æå–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å›°é›£é—œéµå­—æå–å¤±æ•—ï¼š{e}")
            return False

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("ğŸš€ å®Œæ•´æ–°èè™•ç†æµæ°´ç·š")
    print("="*50)
    
    # æª¢æŸ¥ API Key
    api_key = NewsProcessorConfig.get_gemini_api_key()
    if not api_key:
        print("âŒ æœªè¨­å®š GEMINI_API_KEY")
        print("è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®š GEMINI_API_KEY=your_api_key")
        return
    
    print("âœ… API Key å·²è¨­å®š")

    
    try:
        # å‰µå»ºæµæ°´ç·š
        pipeline = CompletePipeline(api_key=api_key)
        
        # åŸ·è¡Œå®Œæ•´æµæ°´ç·š
        generator_result = pipeline.run_complete_pipeline()
        
        if generator_result:
            print("\nğŸ‰ æµæ°´ç·šåŸ·è¡ŒæˆåŠŸï¼")
            print(f"ğŸ“„ æœ€çµ‚è¼¸å‡ºï¼š{generator_result}")
        else:
            print("\nâŒ æµæ°´ç·šåŸ·è¡Œå¤±æ•—")
            
    except Exception as e:
        logger.error(f"âŒ ä¸»ç¨‹å¼åŸ·è¡Œå¤±æ•—ï¼š{e}")
        print(f"\nâŒ åŸ·è¡Œå¤±æ•—ï¼š{e}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        print(f"\nâŒ ç¨‹å¼åŸ·è¡Œå¤±æ•—ï¼š{e}")
