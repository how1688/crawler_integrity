from fileinput import filename
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from bs4 import BeautifulSoup
from webdriver_manager.chrome import ChromeDriverManager
import time
import datetime as dt
from datetime import datetime, timedelta
import requests
from supabase import create_client, Client
import uuid
import os
import json
import random
import re
from urllib.parse import urljoin, urlparse
from collections import defaultdict
from dateutil import parser
from google import genai
from google.genai import types
import shutil
import logging
# Supabase imports
from supabase import create_client, Client
from dotenv import load_dotenv
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities

load_dotenv()

download_dir = "/tmp/downloads"
os.makedirs(download_dir, exist_ok=True)

chrome_bin = os.environ.get("CHROME_BIN")
driver_bin = os.environ.get("CHROMEDRIVER_BIN")

# Supabase é…ç½®
SUPABASE_URL = os.getenv("SUPABASE_URL")  # æ›¿æ›ç‚ºä½ çš„ Supabase URL
SUPABASE_KEY = os.getenv("SUPABASE_KEY")  # æ›¿æ›ç‚ºä½ çš„ Supabase API Key

# åˆå§‹åŒ– Supabase å®¢æˆ¶ç«¯
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    raise ValueError("è«‹å…ˆè¨­å®šä½ çš„ GEMINI_API_KEY ç’°å¢ƒè®Šæ•¸ã€‚")

try:
    gemini_client = genai.Client()
except Exception as e:
    raise ValueError(f"ç„¡æ³•åˆå§‹åŒ– Gemini Clientï¼Œè«‹æª¢æŸ¥ API é‡‘é‘°ï¼š{e}")

def clean_data(data):
    for i, article in enumerate(data):
            print(f"â¡ï¸ æ­£åœ¨è™•ç†ç¬¬ {i+1} ç¯‡æ–‡ç« ...")
            if "articles" in article:
                for j, sub_article in enumerate(article["articles"]):
                    print(f"   â¡ï¸ æ­£åœ¨è™•ç†ç¬¬ {j+1} ç¯‡å­æ–‡ç« ...")

                    # (1) å»é™¤ HTML
                    raw_content = sub_article.get("content", "")
                    soup = BeautifulSoup(raw_content, "html.parser")
                    cleaned_text = soup.get_text(separator="\n", strip=True)

                    # (2) ä½¿ç”¨ Gemini API å»é™¤é›œè¨Š
                    prompt = f"""
                    è«‹å»é™¤ä»¥ä¸‹æ–‡ç« ä¸­çš„é›œè¨Šï¼Œä¾‹å¦‚å¤šé¤˜çš„æ¨™é¡Œã€æ™‚é–“æˆ³è¨˜ã€ä¾†æºè³‡è¨Šç­‰ï¼Œä¸¦æœ€å¤§é‡çš„ä¿ç•™æ‰€æœ‰æ–°èå…§å®¹ï¼š

                    {cleaned_text}

                    ä½ åªéœ€è¦å›è¦†ç¶“éè™•ç†çš„å…§å®¹ï¼Œä¸éœ€è¦ä»»ä½•å…¶ä»–èªªæ˜æˆ–æ¨™é¡Œã€‚
                    å¦‚æœæ²’æœ‰æ–‡ç« å…§å®¹ï¼Œè«‹å›è¦† "[æ¸…æ´—å¤±æ•—]"ã€‚
                    """
                    
                    max_retries = 3  # è¨­å®šæœ€å¤§é‡è©¦æ¬¡æ•¸
                    retries = 0
                    success = False
                    
                    while not success and retries < max_retries:
                        try:
                            # çµ±ä¸€ä½¿ç”¨ client çš„ generate_content æ–¹æ³•
                            response = gemini_client.models.generate_content(
                                model="gemini-2.0-flash",
                                contents=prompt
                            )
                            # ç²å–å›è¦†å…§å®¹çš„æ–¹å¼
                            sub_article["content"] = response.candidates[0].content.parts[0].text.strip()
                            success = True  # è«‹æ±‚æˆåŠŸï¼Œè·³å‡ºè¿´åœˆ
                            time.sleep(1) # æˆåŠŸå¾Œé‚„æ˜¯ç¦®è²Œæ€§åœ°ç¨ç­‰ä¸€ä¸‹
                        except Exception as e:
                            if "503 UNAVAILABLE" in str(e):
                                retries += 1
                                print(f"âš ï¸ åµæ¸¬åˆ°æ¨¡å‹éè¼‰ï¼Œæ­£åœ¨å˜—è©¦ç¬¬ {retries} æ¬¡é‡è©¦...")
                                time.sleep(3 * retries) # æ¯æ¬¡é‡è©¦ç­‰å¾…æ›´ä¹…
                            else:
                                print(f"âŒ ç™¼ç”ŸéŒ¯èª¤æ–¼æ–‡ç« ï¼š{filename}ï¼ŒéŒ¯èª¤è¨Šæ¯ï¼š{e}")
                                sub_article["content"] = "[æ¸…æ´—å¤±æ•—]"
                                break # å…¶ä»–éŒ¯èª¤ç›´æ¥è·³å‡º
                    
                    if not success:
                        print(f"âŒ å˜—è©¦ {max_retries} æ¬¡å¾Œä»ç„¡æ³•æˆåŠŸè™•ç†æ–‡ç« ï¼š{filename}")
                        sub_article["content"] = "[æ¸…æ´—å¤±æ•—]"

    return data

def create_robust_driver(headless: bool = False):
    """å‰µå»ºä¸€å€‹æ›´ç©©å¥çš„ WebDriver"""
    options = webdriver.ChromeOptions()

    if headless:
        options.add_argument("--headless=new")  # ç„¡é ­æ¨¡å¼
    else:
        # æœ‰è¦–çª— â†’ ä¸è¦åŠ  headless
        # options.add_argument("--start-maximized")
        options.add_argument("--headless=new")   # Headless æ¨¡å¼ (æ–°ç‰ˆ Chrome)
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-web-security")
    options.add_argument("--disable-features=VizDisplayCompositor")
    options.add_argument("--page-load-strategy=eager")

    # ç”¨æˆ¶ä»£ç†
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36")

    # é˜²æ­¢è¢«è­˜åˆ¥ç‚ºè‡ªå‹•åŒ–
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)

    # å»£å‘Šå’Œè¿½è¹¤é˜»æ“‹
    options.add_argument("--disable-background-timer-throttling")
    options.add_argument("--disable-backgrounding-occluded-windows")
    options.add_argument("--disable-renderer-backgrounding")
    options.add_argument("--disable-features=TranslateUI")
    options.add_argument("--disable-ipc-flooding-protection")

    # åœ–ç‰‡å’Œåª’é«”å„ªåŒ–
    options.add_argument("--disable-background-media")
    options.add_argument("--disable-background-downloads")
    options.add_argument("--aggressive-cache-discard")
    options.add_argument("--disable-sync")

    # ç¶²è·¯å„ªåŒ–
    options.add_argument("--disable-default-apps")
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-plugins")
    options.add_argument("--disable-notifications")
    options.add_argument("--disable-popup-blocking")

    # è¨˜æ†¶é«”å’Œæ•ˆèƒ½å„ªåŒ–
    options.add_argument("--memory-pressure-off")
    options.add_argument("--max_old_space_size=4096")
    options.add_argument("--single-process")
    options.add_argument("--no-zygote")

    # options.binary_location = chrome_bin   # å‘Šè¨´ Selenium å»ç”¨ Chromium
    # é˜»æ“‹ç‰¹å®šå…§å®¹é¡å‹
    prefs = {
        "download.default_directory": download_dir,
        "download.prompt_for_download": False,
        "directory_upgrade": True,

        # é˜»æ“‹é€šçŸ¥ã€æ’ä»¶ã€å½ˆçª—ã€åœ°ç†ä½ç½®ã€æ”å½±æ©Ÿ/éº¥å…‹é¢¨
        "profile.default_content_setting_values.notifications": 2,
        "profile.default_content_setting_values.plugins": 2,
        "profile.default_content_setting_values.popups": 2,
        "profile.default_content_setting_values.geolocation": 2,
        "profile.default_content_setting_values.media_stream": 2,

        # é˜»æ“‹åœ–ç‰‡
        "profile.managed_default_content_settings.images": 2,

        # é˜»æ“‹å½ˆçª—
        "profile.default_content_settings.popups": 2,
    }
    options.add_experimental_option("prefs", prefs)

    try:
        driver = webdriver.Remote(
            command_executor='https://selenium-hub-production-28a1.up.railway.app/wd/hub',       
            options=options
        )

        # è¨­å®š headless æ¨¡å¼
        params = {
            "behavior": "allow",
            "downloadPath": "/tmp/downloads"
        }
        driver.execute_cdp_cmd("Page.setDownloadBehavior", params)

        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        return driver
    except Exception as e:
        print(f"âŒ å‰µå»º WebDriver å¤±æ•—: {e}")
        raise

def get_main_story_links(main_url, category):
    """æ­¥é©Ÿ 1: å¾ä¸»é æŠ“å–æ‰€æœ‰ä¸»è¦æ•…äº‹é€£çµ"""
    driver = None
    story_links = []
    
    try:
        driver = create_robust_driver(headless=True)
        print(f"ğŸ” æ­£åœ¨æŠ“å– {category} é ˜åŸŸçš„ä¸»è¦æ•…äº‹é€£çµ...")
        driver.get(main_url)
        
        wait = WebDriverWait(driver, 15)
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'c-wiz[jsrenderer="jeGyVb"]')))
        
        soup = BeautifulSoup(driver.page_source, "html.parser")
        c_wiz_blocks = soup.find_all("c-wiz", {"jsrenderer": "jeGyVb"})
        
        print(f"âœ… æ‰¾åˆ° {len(c_wiz_blocks)} å€‹ c-wiz å€å¡Š")
        
        for i, block in enumerate(c_wiz_blocks, start=1):
            try:
                story_link = block.find("a", class_="jKHa4e")
                
                if story_link:
                    href = story_link.get("href")
                    title = story_link.text.strip()
                    
                    if href:
                        if href.startswith("./"):
                            full_link = "https://news.google.com" + href[1:]
                        else:
                            full_link = "https://news.google.com" + href
                        
                        # æª¢æŸ¥è³‡æ–™åº«
                        should_skip, action_type, story_data, skip_reason = check_story_exists_in_supabase(
                            full_link, category, "", ""
                        )
                        
                        print(f"   è™•ç†æ•…äº‹ {i}: {href}")
                        print(f"   ğŸ“‹ æª¢æŸ¥çµæœ: {skip_reason}")
                        
                        # æ ¹æ“šaction_typeæ±ºå®šstory_id
                        if action_type == "add_to_existing_story" and story_data:
                            story_id = story_data["story_id"]
                        else:
                            story_id = str(uuid.uuid4())
                        
                        story_links.append({
                            "index": i,
                            "story_id": story_id,
                            "title": title,
                            "url": full_link,
                            "category": category,
                            "action_type": action_type,
                            "existing_story_data": story_data
                        })
                        
                        print(f"{i}. ğŸ“° [{category}] {title}")
                        print(f"   ğŸ†” æ•…äº‹ID: {story_id}")
                        print(f"   ğŸ”— {full_link}")
                        print(f"   ğŸ¯ è™•ç†é¡å‹: {action_type}")
                        
            except Exception as e:
                print(f"âŒ è™•ç†æ•…äº‹å€å¡Š {i} æ™‚å‡ºéŒ¯: {e}")
                continue
        
        print(f"\nğŸ“Š ç¸½å…±æ”¶é›†åˆ° {len(story_links)} å€‹ {category} é ˜åŸŸéœ€è¦è™•ç†çš„ä¸»è¦æ•…äº‹é€£çµ")
        
    except TimeoutException:
        print(f"âŒ é é¢è¼‰å…¥è¶…æ™‚: {main_url}")
    except WebDriverException as e:
        print(f"âŒ WebDriver éŒ¯èª¤: {e}")
    except Exception as e:
        print(f"âŒ æŠ“å–ä¸»è¦æ•…äº‹é€£çµæ™‚å‡ºéŒ¯: {e}")
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass
    
    return story_links

def get_article_links_from_story(story_info):
    """
    æ­¥é©Ÿ 2: é€²å…¥æ¯å€‹æ•…äº‹é é¢ï¼Œæ‰¾å‡ºæ‰€æœ‰ article ä¸‹çš„æ–‡ç« é€£çµå’Œç›¸é—œä¿¡æ¯
    å¢åŠ æ—¥æœŸéæ¿¾åŠŸèƒ½
    """
    driver = None
    article_links = []
    
    try:
        driver = create_robust_driver(headless=True)
        print(f"\nğŸ” æ­£åœ¨è™•ç†æ•…äº‹ {story_info['index']}: [{story_info['category']}] {story_info['title']}")
        print(f"   ğŸ†” æ•…äº‹ID: {story_info['story_id']}")
        
        # å–å¾—ç¾æœ‰æ•…äº‹çš„ crawl_date (å¦‚æœæœ‰çš„è©±)
        existing_story_data = story_info.get('existing_story_data')
        cutoff_date = None
        if existing_story_data and existing_story_data.get('crawl_date'):
            try:
                cutoff_date_str = existing_story_data['crawl_date']
                if isinstance(cutoff_date_str, str):
                    try:
                        cutoff_date = parser.parse(cutoff_date_str)
                    except:
                        cutoff_date = datetime.strptime(cutoff_date_str, "%Y/%m/%d %H:%M")
                print(f"   ğŸ“… åªè™•ç† {cutoff_date_str} ä¹‹å¾Œçš„æ–‡ç« ")
            except Exception as e:
                print(f"   âš ï¸ è§£æ cutoff_date æ™‚å‡ºéŒ¯: {e}")
        
        driver.get(story_info['url'])
        time.sleep(random.randint(3, 6))
        
        soup = BeautifulSoup(driver.page_source, "html.parser")
        article_elements = soup.find_all("article", class_="MQsxIb xTewfe tXImLc R7GTQ keNKEd keNKEd VkAdve GU7x0c JMJvke q4atFc")
        
        print(f"   âœ… æ‰¾åˆ° {len(article_elements)} å€‹ article å…ƒç´ ")
        
        processed_count = 0
        
        for j, article in enumerate(article_elements, start=1):
            try:
                if processed_count >= 10 :
                    break
                
                h4_element = article.find("h4", class_="ipQwMb ekueJc RD0gLb")
                
                if h4_element:
                    link = h4_element.find("a", class_="DY5T1d RZIKme")
                    
                    if link:
                        href = link.get("href")
                        link_text = link.text.strip()
                        
                        media_element = article.find("a", class_="wEwyrc")
                        media = media_element.text.strip() if media_element else "æœªçŸ¥ä¾†æº"

                        # è·³éç‰¹å®šåª’é«”
                        if media in ["MSN", "è‡ªç”±æ™‚å ±", "chinatimes.com", "ä¸­æ™‚é›»å­å ±", 
                                     "ä¸­æ™‚æ–°èç¶²", "ä¸Šå ±Up Media", "é»æ–°è", "é¦™æ¸¯æ–‡åŒ¯ç¶²", 
                                     "å¤©ä¸‹é›œèªŒ", "è‡ªç”±å¥åº·ç¶²", "çŸ¥æ–°è", "SUPERMOTO8", 
                                     "è­¦æ”¿æ™‚å ±", "å¤§ç´€å…ƒ", "æ–°å”äººé›»è¦–å°", "arch-web.com.tw",
                                     "éŸ“è¯ç¤¾", "å…¬è¦–æ–°èç¶²PNN", "å„ªåˆ†æUAnalyze", "AASTOCKS.com",
                                     "KSD éŸ“æ˜Ÿç¶²", "å•†å‘¨", "è‡ªç”±è²¡ç¶“", "é‰…äº¨è™Ÿ",
                                     "wownews.tw", "utravel.com.hk", "æ›´ç”Ÿæ–°èç¶²", "é¦™æ¸¯é›»å°",
                                     "citytimes.tw"]:
                            continue

                        time_element = article.find(class_="WW6dff uQIVzc Sksgp slhocf")
                        article_datetime = "æœªçŸ¥æ™‚é–“"
                        
                        if time_element and time_element.get("datetime"):
                            dt_str = time_element.get("datetime")
                            dt_obj = datetime.fromisoformat(dt_str.replace("Z", "+00:00"))
                            article_datetime_obj = dt_obj + timedelta(hours=8)
                            article_datetime = article_datetime_obj.strftime("%Y/%m/%d %H:%M:%S")
                            
                            # **é‡è¦ï¼šæª¢æŸ¥æ–‡ç« æ™‚é–“æ˜¯å¦åœ¨ cutoff_date ä¹‹å¾Œ**
                            if cutoff_date and article_datetime_obj <= cutoff_date:
                                print(f"     â­ï¸  è·³éèˆŠæ–‡ç« : {link_text}")
                                print(f"        æ–‡ç« æ™‚é–“: {article_datetime} <= æˆªæ­¢æ™‚é–“: {cutoff_date}")
                                continue
                        
                        if href:
                            if href.startswith("./"):
                                full_href = "https://news.google.com" + href[1:]
                            else:
                                full_href = "https://news.google.com" + href
                            
                            # æª¢æŸ¥æ–‡ç« æ˜¯å¦éœ€è¦è™•ç†
                            should_skip, action_type, story_data, skip_reason = check_story_exists_in_supabase(
                                story_info['url'], story_info['category'], article_datetime, full_href
                            )
                            
                            if should_skip and action_type == "skip":
                                print(f"     â­ï¸  è·³éæ–‡ç« : {link_text}")
                                print(f"        åŸå› : {skip_reason}")
                                continue
                            
                            article_links.append({
                                "story_id": story_info['story_id'],
                                "story_title": story_info['title'],
                                "story_category": story_info['category'],
                                "story_url": story_info['url'],
                                "article_index": processed_count + 1,
                                "article_title": link_text,
                                "article_url": full_href,
                                "media": media,
                                "article_datetime": article_datetime,
                                "action_type": action_type,
                                "existing_story_data": story_data
                            })
                            
                            processed_count += 1
                            print(f"     {processed_count}. ğŸ“„ {link_text}")
                            print(f"        ğŸ¢ åª’é«”: {media}")
                            print(f"        ğŸ“… æ™‚é–“: {article_datetime}")
                            print(f"        ğŸ¯ è™•ç†é¡å‹: {action_type}")
                            print(f"        ğŸ”— {full_href}")
                            
            except Exception as e:
                print(f"     âŒ è™•ç†æ–‡ç« å…ƒç´  {j} æ™‚å‡ºéŒ¯: {e}")
                continue
        
        if processed_count == 0 and cutoff_date:
            print(f"   â„¹ï¸  æ­¤æ•…äº‹æ²’æœ‰ {cutoff_date} ä¹‹å¾Œçš„æ–°æ–‡ç« ")
        
    except Exception as e:
        print(f"âŒ è™•ç†æ•…äº‹æ™‚å‡ºéŒ¯: {e}")
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass
    
    return article_links

def get_final_content(article_info, driver):
    """
    æ­¥é©Ÿ 3: è·³è½‰åˆ°åŸå§‹ç¶²ç«™ä¸¦æŠ“å–å…§å®¹ - æ”¹é€²éŒ¯èª¤è™•ç†
    """
    MAX_RETRIES = 2
    TIMEOUT = 15
    
    for attempt in range(MAX_RETRIES):
        try:
            print(f"   å˜—è©¦ç¬¬ {attempt + 1} æ¬¡è¨ªå•...")
            
            # æª¢æŸ¥ driver æ˜¯å¦ä»ç„¶å¯ç”¨
            try:
                driver.set_page_load_timeout(TIMEOUT)
            except Exception as e:
                print(f"   âŒ WebDriver è¨­ç½®è¶…æ™‚å¤±æ•—: {e}")
                return None
            
            try:
                driver.get(article_info['article_url'])
            except TimeoutException:
                print(f"   âš ï¸ é é¢åŠ è¼‰è¶…æ™‚ï¼Œä½†ç¹¼çºŒå˜—è©¦ç²å–å…§å®¹...")
            except WebDriverException as e:
                print(f"   âŒ WebDriver éŒ¯èª¤: {e}")
                if "chrome not reachable" in str(e).lower() or "session deleted" in str(e).lower():
                    print(f"   ğŸ’€ WebDriver æœƒè©±å·²å¤±æ•ˆ")
                    return None
                if attempt < MAX_RETRIES - 1:
                    print(f"   ğŸ”„ {TIMEOUT//4} ç§’å¾Œé‡è©¦...")
                    time.sleep(TIMEOUT//4)
                    continue
                else:
                    return None
            except Exception as e:
                print(f"   âŒ æœªçŸ¥éŒ¯èª¤: {e}")
                return None
            
            time.sleep(random.randint(3, 6))
            
            try:
                skip_patterns = [
                    "https://www.gamereactor.cn/video",
                    "https://wantrich.chinatimes.com",
                    "https://taongafarm.site", 
                    "https://www.cmoney.tw",
                    "https://www.cw.com.tw",
                    "https://www.msn.com/",
                    "https://cn.wsj.com/",
                    "https://about.pts.org.tw/pr/latestnews",
                    "https://www.chinatimes.com",
                    "https://sports.ltn.com.tw",
                    "https://video.ltn.com.tw",
                    "https://def.ltn.com.tw",
                    "https://www.upmedia.mg",
                    "http://www.aastocks.com",
                    "https://news.futunn.com",
                    "https://ec.ltn.com.tw/",
                    "https://health.ltn.com.tw",
                    "https://www.taiwannews",
                    "https://www.ftvnews.com.tw",
                    "https://tw.nextapple.com",
                    "https://talk.ltn.com.tw",
                    "https://www.mobile01.com/",
                    "https://www.worldjournal.com/"
                ]
                
                try:
                    final_url = driver.current_url
                    print(f"   æœ€çµ‚ç¶²å€: {final_url}")
                except Exception as e:
                    print(f"   âš ï¸ ç„¡æ³•ç²å–ç•¶å‰ URL: {e}")
                    final_url = article_info['article_url']
                
                if final_url.startswith("https://www.google.com/sorry/index?continue=https://news.google.com/read"):
                    print(f"   âš ï¸ é‡åˆ° Google é©—è­‰é é¢ï¼Œå˜—è©¦åˆ·æ–°...")
                    try:
                        driver.refresh()
                        time.sleep(random.randint(2, 4))
                        final_url = driver.current_url
                    except:
                        print(f"   âŒ åˆ·æ–°å¤±æ•—")
                        return None
                        
                elif any(final_url.startswith(pattern) for pattern in skip_patterns):
                    print(f"   â­ï¸  è·³éé€£çµ: {final_url}")
                    return None
                
            except WebDriverException as e:
                print(f"   âŒ ç²å– URL æ™‚å‡ºéŒ¯: {e}")
                if "chrome not reachable" in str(e).lower():
                    return None
                final_url = article_info['article_url']
            
            try:
                html = driver.page_source
                if not html or len(html) < 100:  # æª¢æŸ¥é é¢å…§å®¹æ˜¯å¦æœ‰æ•ˆ
                    print(f"   âš ï¸ é é¢å…§å®¹éçŸ­æˆ–ç‚ºç©º")
                    if attempt < MAX_RETRIES - 1:
                        continue
                    else:
                        return None
                        
                soup = BeautifulSoup(html, "html.parser")
            except WebDriverException as e:
                print(f"   âŒ ç„¡æ³•ç²å–é é¢æºç¢¼: {e}")
                if "chrome not reachable" in str(e).lower():
                    return None
                if attempt < MAX_RETRIES - 1:
                    print(f"   ğŸ”„ {TIMEOUT//2} ç§’å¾Œé‡è©¦...")
                    time.sleep(TIMEOUT//2)
                    continue
                else:
                    return None
            except Exception as e:
                print(f"   âŒ è§£æé é¢æ™‚å‡ºéŒ¯: {e}")
                return None

            # å…§å®¹æå–é‚è¼¯ï¼ˆä¿æŒåŸæœ‰é‚è¼¯ï¼‰
            content_to_clean = None
            article_tag = soup.find('article')
            if article_tag and article_info['media'] != 'Now æ–°è':
                content_to_clean = str(article_tag)
            elif soup.find('artical'):
                article_tag = soup.find('artical')
                content_to_clean = str(article_tag)
            else:
                target_ids = [
                    'text ivu-mt', 'content-box', 'text', 'boxTitle', 
                    'news-detail-content', 'story', 'article-content__editor', 'article-body', 
                    'artical-content', 'article_text', 'newsText'
                ]
                
                div_by_id = None
                for target_id in target_ids:
                    try:
                        div_by_id = soup.find('div', id=target_id)
                        if div_by_id:
                            break
                    except Exception as e:
                        continue
                
                if div_by_id:
                    content_to_clean = str(div_by_id)
                else:
                    target_classes = ['articleBody clearfix', 'text boxTitle','text ivu-mt', 'paragraph', 'atoms', 
                                      'news-box-text border', 'newsLeading', 'text']

                    div_by_class = None
                    for target_class in target_classes:
                        try:
                            div_by_class = soup.find('div', class_=target_class)
                            if div_by_class:
                                break
                        except Exception as e:
                            continue
                    
                    if div_by_class:
                        content_to_clean = str(div_by_class)
                    else:
                        if soup.body:
                            content_to_clean = str(soup.body)

            if content_to_clean:
                try:
                    content_soup = BeautifulSoup(content_to_clean, "html.parser")
                    
                    excluded_divs = content_soup.find_all('div', class_='paragraph moreArticle')
                    for div in excluded_divs:
                        div.decompose()
                    
                    excluded_p_classes = [
                        'mb-module-gap read-more-vendor break-words leading-[1.4] text-px20 lg:text-px18 lg:leading-[1.8] text-batcave __web-inspector-hide-shortcut__',
                        'mb-module-gap read-more-editor break-words leading-[1.4] text-px20 lg:text-px18 lg:leading-[1.8] text-batcave'
                    ]
                    
                    for p_class in excluded_p_classes:
                        excluded_ps = content_soup.find_all('p', class_=p_class)
                        for p in excluded_ps:
                            p.decompose()
                    
                    body_content = str(content_soup)
                    body_content = body_content.replace("\x00", "").replace("\r", "").replace("\n", "")
                    body_content = body_content.replace('"', '\\"')
                    
                except Exception as e:
                    print(f"   âŒ å…§å®¹æ¸…ç†æ™‚å‡ºéŒ¯: {e}")
                    body_content = ""
            else:
                body_content = ""
                print(f"   âš ï¸ æœªæ‰¾åˆ°å¯ç”¨çš„å…§å®¹")
                
            article_id = str(uuid.uuid4())

            if ("æ‚¨çš„ç¶²è·¯å·²é­åˆ°åœæ­¢è¨ªå•æœ¬ç¶²ç«™çš„æ¬Šåˆ©ã€‚" in body_content or 
                "æˆ‘å€‘çš„ç³»çµ±åµæ¸¬åˆ°æ‚¨çš„é›»è…¦ç¶²è·¯é€å‡ºçš„æµé‡æœ‰ç•°å¸¸æƒ…æ³ã€‚" in body_content):
                print(f"   âš ï¸ æ–‡ç«  {article_id} è¢«å°é–ï¼Œç„¡æ³•è¨ªå•")
                return None

            return {
                "story_id": article_info['story_id'],
                "story_title": article_info['story_title'],
                "story_category": article_info['story_category'],
                "story_url": article_info['story_url'],
                "id": article_id,
                "article_index": article_info['article_index'],
                "article_title": article_info['article_title'],
                "google_news_url": article_info['article_url'],
                "final_url": final_url,
                "media": article_info.get('media', 'æœªçŸ¥ä¾†æº'),
                "content": body_content,
                "article_datetime": article_info.get('article_datetime', 'æœªçŸ¥æ™‚é–“'),
                "action_type": article_info.get('action_type', 'process'),
                "existing_story_data": article_info.get('existing_story_data')
            }
            
        except Exception as e:
            print(f"   âŒ ç¬¬ {attempt + 1} æ¬¡å˜—è©¦å¤±æ•—: {e}")
            if "chrome not reachable" in str(e).lower():
                print(f"   ğŸ’€ Chrome ç€è¦½å™¨ç„¡æ³•é€£æ¥ï¼Œè¿”å› None")
                return None
            if attempt < MAX_RETRIES - 1:
                print(f"   ğŸ”„ {TIMEOUT//2} ç§’å¾Œé‡è©¦...")
                time.sleep(TIMEOUT//2)
            else:
                print(f"   ğŸ’€ å·²é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œæ”¾æ£„è©²æ–‡ç« ")
    
    return None

def check_story_exists_in_supabase(story_url, category, article_datetime="", article_url=""):
    """
    æª¢æŸ¥æ•…äº‹æ˜¯å¦å­˜åœ¨æ–¼æ•¸æ“šåº«ä¸­ï¼Œä¸¦è¿”å›ç›¸æ‡‰çš„è™•ç†é‚è¼¯
    
    Args:
        story_url: æ•…äº‹URL
        category: æ–°èåˆ†é¡
        article_datetime: æ–‡ç« æ™‚é–“
        article_url: æ–‡ç« URL
    
    Returns:
        tuple: (should_skip, action_type, story_data, skip_reason)
    """
    try:
        # 1. æª¢æŸ¥ story_url æ˜¯å¦å­˜åœ¨ï¼ŒæŒ‰ crawl_date é™åºæ’åˆ—å–æœ€æ–°çš„
        story_response = supabase.table("stories").select("*").eq("story_url", story_url).order("crawl_date", desc=True).limit(1).execute()

        if not story_response.data:
            # æ•…äº‹ä¸å­˜åœ¨ï¼Œéœ€è¦å‰µå»ºæ–°æ•…äº‹
            return False, "create_new_story", None, "æ–°æ•…äº‹"
        
        existing_story = story_response.data[0]
        story_id = existing_story["story_id"]
        existing_crawl_date = existing_story["crawl_date"]
        
        # 2. æª¢æŸ¥æ™‚é–“ç¯„åœï¼ˆ3å¤©å…§ï¼‰
        try:
            if existing_crawl_date:
                # è™•ç†ä¸åŒçš„æ—¥æœŸæ ¼å¼
                if isinstance(existing_crawl_date, str):
                    try:
                        existing_dt = parser.parse(existing_crawl_date)
                    except:
                        existing_dt = datetime.strptime(existing_crawl_date, "%Y/%m/%d %H:%M")
                else:
                    existing_dt = existing_crawl_date
                    
                current_dt = datetime.now()
                days_diff = (current_dt - existing_dt).days
                
                if days_diff <= 3:
                    # åœ¨3å¤©å…§ï¼Œä½¿ç”¨ç¾æœ‰æ•…äº‹ID
                    print(f"   ğŸ”„ ä½¿ç”¨ç¾æœ‰æ•…äº‹ID: {story_id} (è·é›¢ä¸Šæ¬¡çˆ¬å– {days_diff} å¤©)")
                    print(f"   ğŸ“… ä¸Šæ¬¡çˆ¬å–æ™‚é–“: {existing_crawl_date}")
                    
                    # 3. æª¢æŸ¥æ–‡ç« æ˜¯å¦åœ¨ crawl_date ä¹‹å¾Œ
                    if article_datetime and article_datetime != "æœªçŸ¥æ™‚é–“":
                        try:
                            article_dt = parser.parse(article_datetime)
                            
                            # æ¯”è¼ƒæ–‡ç« æ™‚é–“å’Œä¸Šæ¬¡çˆ¬å–æ™‚é–“
                            if article_dt <= existing_dt:
                                # æ–‡ç« æ™‚é–“æ—©æ–¼æˆ–ç­‰æ–¼ä¸Šæ¬¡çˆ¬å–æ™‚é–“ï¼Œè·³é
                                return True, "skip", existing_story, f"æ–‡ç« æ™‚é–“ {article_datetime} æ—©æ–¼ä¸Šæ¬¡çˆ¬å–æ™‚é–“ {existing_crawl_date}"
                                
                        except Exception as date_parse_error:
                            print(f"   âš ï¸ æ–‡ç« æ™‚é–“è§£æéŒ¯èª¤: {date_parse_error}")
                            # å¦‚æœç„¡æ³•è§£ææ–‡ç« æ™‚é–“ï¼Œç¹¼çºŒæª¢æŸ¥ URL
                    
                    # 4. æª¢æŸ¥æ–‡ç« URLæ˜¯å¦å·²å­˜åœ¨
                    if article_url:
                        article_response = supabase.table("cleaned_news").select("article_id").eq("article_url", article_url).execute()
                        
                        if article_response.data:
                            # æ–‡ç« å·²å­˜åœ¨ï¼Œè·³é
                            return True, "skip", existing_story, f"æ–‡ç« å·²å­˜åœ¨æ–¼æ•…äº‹ {story_id}"
                        else:
                            # æ–‡ç« ä¸å­˜åœ¨ä¸”æ™‚é–“ç¬¦åˆï¼ŒåŠ å…¥ç¾æœ‰æ•…äº‹
                            return False, "add_to_existing_story", existing_story, f"åŠ å…¥ç¾æœ‰æ•…äº‹ {story_id} (æ–°æ–‡ç« )"
                    else:
                        # æ²’æœ‰æ–‡ç« URLï¼ˆæ•…äº‹å±¤ç´šçš„æª¢æŸ¥ï¼‰
                        return False, "add_to_existing_story", existing_story, f"ä½¿ç”¨ç¾æœ‰æ•…äº‹ {story_id}"
                else:
                    # è¶…é3å¤©ï¼Œå‰µå»ºæ–°æ•…äº‹
                    return False, "create_new_story", None, f"è¶…éæ™‚é–“é™åˆ¶ ({days_diff} å¤©)ï¼Œå‰µå»ºæ–°æ•…äº‹"
            else:
                # æ²’æœ‰ crawl_dateï¼Œå‰µå»ºæ–°æ•…äº‹
                return False, "create_new_story", None, "ç¼ºå°‘çˆ¬å–æ—¥æœŸï¼Œå‰µå»ºæ–°æ•…äº‹"
                
        except Exception as date_error:
            print(f"   âš ï¸ æ—¥æœŸè§£æéŒ¯èª¤: {date_error}")
            return False, "create_new_story", None, f"æ—¥æœŸè§£æéŒ¯èª¤: {date_error}"
            
    except Exception as e:
        print(f"   âŒ æª¢æŸ¥Supabaseæ™‚å‡ºéŒ¯: {e}")
        return False, "create_new_story", None, f"è³‡æ–™åº«æª¢æŸ¥éŒ¯èª¤: {e}"


def save_story_to_supabase(story_data):
    """
    ä¿å­˜æ•…äº‹åˆ° Supabase stories è¡¨
    """
    try:
        story_record = {
            "story_id": story_data["story_id"],
            "story_url": story_data["story_url"],
            "story_title": story_data["story_title"],
            "category": story_data["category"],
            "crawl_date": story_data["crawl_date"]
        }
        
        # ä½¿ç”¨ upsert ä¾†é¿å…é‡è¤‡æ’å…¥
        response = supabase.table("stories").upsert(story_record, on_conflict="story_id").execute()
        print(f"   âœ… æ•…äº‹å·²ä¿å­˜åˆ°è³‡æ–™åº«: {story_data['story_id']}")
        return True
        
    except Exception as e:
        print(f"   âŒ ä¿å­˜æ•…äº‹åˆ°è³‡æ–™åº«å¤±æ•—: {e}")
        return False

def save_article_to_supabase(article_data, story_id):
    """
    ä¿å­˜æ–‡ç« åˆ° Supabase cleaned_news è¡¨
    """
    try:
        article_record = {
            "article_id": article_data["article_id"],
            "article_title": article_data["article_title"],
            "article_url": article_data["article_url"],
            "content": article_data["content"],
            "media": article_data["media"],
            "story_id": story_id
            }
            
        # ä½¿ç”¨ upsert ä¾†é¿å…é‡è¤‡æ’å…¥
        article_url = article_data["article_url"]
        existing_article = supabase.table("cleaned_news").select("article_id").eq("article_url", article_url).execute()
        if existing_article.data:
            print(f"   âš ï¸ æ–‡ç« å·²å­˜åœ¨ï¼Œè·³éä¿å­˜: {article_data['article_id']}")
            return True
        elif not article_data["content"] or "[æ¸…æ´—å¤±æ•—]" in article_data["content"] or "è«‹æä¾›" in article_data["content"]:
            print(f"   âš ï¸ æ–‡ç« å…§å®¹ç„¡æ•ˆï¼Œè·³éä¿å­˜: {article_data['article_id']}")
            return True
        response = supabase.table("cleaned_news").upsert(article_record, on_conflict="article_id").execute()
        print(f"   âœ… æ–‡ç« å·²ä¿å­˜åˆ°è³‡æ–™åº«: {article_data['article_id']}")
        return True
        
    except Exception as e:
        print(f"   âŒ ä¿å­˜æ–‡ç« åˆ°è³‡æ–™åº«å¤±æ•—: {e}")
        return False

def group_articles_by_story_and_time(processed_articles, time_window_days=3):
    """
    æ ¹æ“šæ•…äº‹åˆ†çµ„ï¼Œç„¶å¾Œåœ¨æ¯å€‹æ•…äº‹å…§æŒ‰æ™‚é–“å°‡æ–‡ç« åˆ†çµ„
    åŒæ™‚æ”¯æ´ç¾æœ‰æ•…äº‹çš„æ›´æ–°åŠŸèƒ½
    
    Args:
        processed_articles: å¾ get_final_content è™•ç†å¾Œçš„æ–‡ç« åˆ—è¡¨
        time_window_days: æ™‚é–“çª—å£å¤©æ•¸ï¼ˆçœŸæ­£çš„æ¯Nå¤©åˆ†çµ„ï¼‰
        enable_time_grouping: æ˜¯å¦å•Ÿç”¨æ™‚é–“åˆ†çµ„åŠŸèƒ½
    
    Returns:
        list: è™•ç†å¾Œçš„æ•…äº‹åˆ—è¡¨ï¼ŒåŒ…å« action_type æ¬„ä½
    """
    print(f"\n=== é–‹å§‹åŸºæ–¼æ•…äº‹å’Œæ™‚é–“åˆ†çµ„æ–‡ç«  ===")
    print(f"æ™‚é–“çª—å£: {time_window_days}å¤©")
    
    # æŒ‰æ•…äº‹IDåˆ†çµ„
    story_grouped = defaultdict(list)
    for article in processed_articles:
        story_id = article["story_id"]
        story_grouped[story_id].append(article)
    
    all_final_stories = []
    
    for story_id, articles in story_grouped.items():
        if not articles:
            continue
            
        # ç²å–æ•…äº‹åŸºæœ¬ä¿¡æ¯ï¼ˆå¾ç¬¬ä¸€ç¯‡æ–‡ç« ï¼‰
        first_article = articles[0]
        story_title = first_article["article_title"]
        story_url = first_article["story_url"]
        story_category = first_article["story_category"]
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºç¾æœ‰æ•…äº‹æ›´æ–°
        existing_story_data = first_article.get("existing_story_data")
        is_existing_story = existing_story_data and first_article.get("action_type") == "add_to_existing_story"
        
        if is_existing_story:
            print(f"\nğŸ”„ æ›´æ–°ç¾æœ‰æ•…äº‹: {story_title}")
            print(f"   ğŸ†” Story ID: {story_id}")
            print(f"   ğŸ“… åŸæœ‰ Crawl Date: {existing_story_data.get('crawl_date', 'æœªçŸ¥')}")
            print(f"   ğŸ“… åŸæœ‰æ™‚é–“ç¯„åœ: {existing_story_data.get('time_range', 'æœªçŸ¥')}")
            base_action_type = "update_existing_story"
        else:
            print(f"\nğŸ†• è™•ç†æ–°æ•…äº‹: {story_title}")
            print(f"   ğŸ†” Story ID: {story_id}")
            base_action_type = "create_new_story"
        
        print(f"   ğŸ“Š åŒ…å« {len(articles)} ç¯‡æ–‡ç« ")
        
        # è§£ææ‰€æœ‰æ–‡ç« çš„æ™‚é–“
        articles_with_time = []
        for article in articles:
            article_datetime = article.get('article_datetime', 'æœªçŸ¥æ™‚é–“')
            if article_datetime and article_datetime != 'æœªçŸ¥æ™‚é–“':
                try:
                    parsed_dt = parser.parse(article_datetime)
                    articles_with_time.append({
                        'article': article,
                        'datetime': parsed_dt
                    })
                except (ValueError, TypeError) as e:
                    print(f"âš ï¸ è§£ææ™‚é–“å¤±æ•—: {article_datetime}, ä½¿ç”¨ç•¶å‰æ™‚é–“")
                    articles_with_time.append({
                        'article': article,
                        'datetime': datetime.now()
                    })
            else:
                # æ²’æœ‰æ™‚é–“çš„æ–‡ç« ä½¿ç”¨ç•¶å‰æ™‚é–“
                articles_with_time.append({
                    'article': article,
                    'datetime': datetime.now()
                })
        
        # æŒ‰æ™‚é–“æ’åº
        articles_with_time.sort(key=lambda x: x['datetime'])
        
        # åŸ·è¡Œæ™‚é–“çª—å£åˆ†çµ„
        time_groups = _create_time_groups(articles_with_time, time_window_days)
        print(f"   ğŸ“Š åœ¨æ•…äº‹å…§åˆ†æˆ {len(time_groups)} å€‹æ™‚é–“çµ„")

        # ç‚ºæ¯å€‹æ™‚é–“çµ„å‰µå»ºæœ€çµ‚çš„æ•…äº‹æ•¸æ“š
        for group_idx, group in enumerate(time_groups):
            # æ‰¾åˆ°çµ„å…§æœ€æ—©å’Œæœ€æ™šçš„æ™‚é–“
            earliest_time = min(item['datetime'] for item in group)
            latest_time = max(item['datetime'] for item in group)
            
            # æ±ºå®šä½¿ç”¨å“ªå€‹æ™‚é–“ä½œç‚º crawl_date
            if is_existing_story:
                # ç¾æœ‰æ•…äº‹ï¼šå„ªå…ˆä½¿ç”¨åŸæœ‰çš„ crawl_dateï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨ç•¶å‰æ™‚é–“
                original_crawl_date = existing_story_data.get('crawl_date')
                if original_crawl_date:
                    crawl_date = original_crawl_date
                    print(f"      ğŸ“… ä¿æŒåŸæœ‰ Crawl Date: {crawl_date}")
                else:
                    crawl_date = datetime.now().strftime("%Y/%m/%d %H:%M")
                    print(f"      ğŸ“… ä½¿ç”¨ç•¶å‰æ™‚é–“ä½œç‚º Crawl Date: {crawl_date}")
            else:
                # æ–°æ•…äº‹ï¼šä½¿ç”¨æœ€æ—©æ–‡ç« æ™‚é–“
                crawl_date = earliest_time.strftime("%Y/%m/%d %H:%M")
            
            # è¨ˆç®—å¯¦éš›çš„æ™‚é–“ç¯„åœ - å°æ–¼ç¾æœ‰æ•…äº‹ï¼Œå„ªå…ˆä½¿ç”¨åŸæœ‰æ™‚é–“ç¯„åœ
            if is_existing_story and existing_story_data.get('time_range'):
                # ç¾æœ‰æ•…äº‹ä¸”æœ‰æ™‚é–“ç¯„åœï¼šåˆä½µæ–°èˆŠæ™‚é–“ç¯„åœ
                original_time_range = existing_story_data.get('time_range')
                try:
                    # è§£æåŸæœ‰æ™‚é–“ç¯„åœ
                    if ' - ' in original_time_range:
                        orig_start_str, orig_end_str = original_time_range.split(' - ')
                        orig_start = datetime.strptime(orig_start_str, '%Y/%m/%d')
                        orig_end = datetime.strptime(orig_end_str, '%Y/%m/%d')
                    else:
                        orig_start = orig_end = datetime.strptime(original_time_range, '%Y/%m/%d')
                    
                    # è¨ˆç®—åˆä½µå¾Œçš„æ™‚é–“ç¯„åœ
                    combined_start = min(orig_start, earliest_time.replace(hour=0, minute=0, second=0, microsecond=0))
                    combined_end = max(orig_end, latest_time.replace(hour=0, minute=0, second=0, microsecond=0))
                    
                    if combined_start.date() == combined_end.date():
                        time_range = combined_start.strftime('%Y/%m/%d')
                    else:
                        time_range = f"{combined_start.strftime('%Y/%m/%d')} - {combined_end.strftime('%Y/%m/%d')}"
                    
                    print(f"      ğŸ“… åˆä½µæ™‚é–“ç¯„åœ: {original_time_range} + {earliest_time.strftime('%Y/%m/%d')}~{latest_time.strftime('%Y/%m/%d')} = {time_range}")
                    
                except (ValueError, TypeError) as e:
                    print(f"      âš ï¸ è§£æåŸæœ‰æ™‚é–“ç¯„åœå¤±æ•—: {original_time_range}ï¼Œä½¿ç”¨æ–°æ–‡ç« æ™‚é–“ç¯„åœ")
                    # å¦‚æœè§£æå¤±æ•—ï¼Œä½¿ç”¨æ–°æ–‡ç« çš„æ™‚é–“ç¯„åœ
                    if earliest_time.date() == latest_time.date():
                        time_range = earliest_time.strftime('%Y/%m/%d')
                    else:
                        time_range = f"{earliest_time.strftime('%Y/%m/%d')} - {latest_time.strftime('%Y/%m/%d')}"
            else:
                # æ–°æ•…äº‹æˆ–ç¾æœ‰æ•…äº‹æ²’æœ‰æ™‚é–“ç¯„åœï¼šä½¿ç”¨æ–°æ–‡ç« çš„æ™‚é–“ç¯„åœ
                if earliest_time.date() == latest_time.date():
                    time_range = earliest_time.strftime('%Y/%m/%d')
                else:
                    time_range = f"{earliest_time.strftime('%Y/%m/%d')} - {latest_time.strftime('%Y/%m/%d')}"
            
            # ç”Ÿæˆæœ€çµ‚çš„æ•…äº‹IDå’Œæ¨™é¡Œ
            if len(time_groups) > 1:
                # å¤šå€‹æ™‚é–“çµ„ï¼šéœ€è¦ç‚ºæ¯çµ„ç”Ÿæˆæ–°çš„ID
                if is_existing_story:
                    # ç¾æœ‰æ•…äº‹åˆ†çµ„ï¼šä¿æŒåŸIDä¸¦æ·»åŠ çµ„åˆ¥å¾Œç¶´
                    base_story_id = story_id
                    final_story_id = f"{base_story_id}_G{group_idx + 1:02d}"
                    final_action_type = f"{base_action_type}_with_time_grouping"
                else:
                    # æ–°æ•…äº‹åˆ†çµ„ï¼šæ¨™æº–çš„åˆ†çµ„é‚è¼¯
                    base_story_id = story_id[:-2] if len(story_id) >= 2 else story_id
                    final_story_id = f"{base_story_id}{group_idx + 1:02d}"
                    final_action_type = f"{base_action_type}_with_time_grouping"
                
                final_story_title = f"{story_title} (ç¬¬{group_idx + 1}çµ„)"
            else:
                # å–®ä¸€çµ„ï¼šä¿æŒåŸæœ‰IDå’Œæ¨™é¡Œ
                final_story_id = story_id
                final_story_title = story_title
                final_action_type = base_action_type
            
            # æº–å‚™æ–‡ç« åˆ—è¡¨
            grouped_articles = []
            for article_idx, item in enumerate(group, 1):
                article = item['article']
                grouped_articles.append({
                    "article_id": article["id"],
                    "article_title": article["article_title"],
                    "article_index": article_idx,  # é‡æ–°ç·¨è™Ÿ
                    "google_news_url": article["google_news_url"],
                    "article_url": article["final_url"],
                    "media": article["media"],
                    "content": article["content"],
                    "original_datetime": article.get("article_datetime", "æœªçŸ¥æ™‚é–“")
                })
            
            # å‰µå»ºæ•…äº‹æ•¸æ“šçµæ§‹
            story_data = {
                "story_id": final_story_id,
                "story_title": final_story_title,
                "story_url": story_url,
                "crawl_date": crawl_date,
                "time_range": time_range,
                "category": story_category,
                "articles": grouped_articles,
                "action_type": final_action_type,
                "is_time_grouped": len(time_groups) > 1,
                "group_index": group_idx + 1 if len(time_groups) > 1 else None,
                "total_groups": len(time_groups) if len(time_groups) > 1 else None
            }
            
            # å¦‚æœæ˜¯ç¾æœ‰æ•…äº‹ï¼Œä¿ç•™æ›´å¤šåŸæœ‰æ•¸æ“šçš„åƒè€ƒ
            if is_existing_story:
                story_data["original_story_data"] = existing_story_data
                story_data["time_range_updated"] = existing_story_data.get('time_range') != time_range
                story_data["crawl_date_preserved"] = existing_story_data.get('crawl_date') == crawl_date
            
            all_final_stories.append(story_data)
            
            # è¨ˆç®—å¯¦éš›å¤©æ•¸è·¨åº¦
            actual_days = (latest_time.date() - earliest_time.date()).days + 1
            
            if len(time_groups) > 1:
                print(f"   ğŸ“° æ™‚é–“çµ„ {group_idx + 1}: {time_range} (å¯¦éš›è·¨åº¦: {actual_days}å¤©)")
            else:
                print(f"   ğŸ“° å®Œæ•´æ•…äº‹: {time_range} (å¯¦éš›è·¨åº¦: {actual_days}å¤©)")
            
            print(f"      ğŸ†” æœ€çµ‚ Story ID: {final_story_id}")
            print(f"      ğŸ“… Crawl Date: {crawl_date}")
            print(f"      ğŸ“„ æ–‡ç« æ•¸: {len(grouped_articles)} ç¯‡")
            print(f"      ğŸ¯ è™•ç†é¡å‹: {final_action_type}")
    
    print(f"\nâœ… ç¸½å…±è™•ç†å®Œæˆ {len(all_final_stories)} å€‹æœ€çµ‚æ•…äº‹")
    return all_final_stories


def _create_time_groups(articles_with_time, time_window_days):
    """
    æ ¹æ“šæ™‚é–“çª—å£å°‡æ–‡ç« åˆ†çµ„çš„å…§éƒ¨å‡½æ•¸
    """
    time_groups = []
    current_group = []
    current_group_start_time = None
    current_group_end_time = None
    
    for item in articles_with_time:
        article_time = item['datetime']
        
        if current_group_start_time is None:
            # ç¬¬ä¸€ç¯‡æ–‡ç« ï¼Œé–‹å§‹ç¬¬ä¸€çµ„
            current_group_start_time = article_time
            current_group_end_time = article_time + timedelta(days=time_window_days)
            current_group.append(item)
            print(f"      ğŸ é–‹å§‹æ–°çµ„: {current_group_start_time.strftime('%Y/%m/%d %H:%M')} - {current_group_end_time.strftime('%Y/%m/%d %H:%M')}")
        else:
            # æª¢æŸ¥æ˜¯å¦åœ¨ç•¶å‰çµ„çš„æ™‚é–“çª—å£å…§
            if article_time < current_group_end_time:
                # åœ¨åŒä¸€çµ„å…§
                current_group.append(item)
                print(f"         âœ… åŠ å…¥ç•¶å‰çµ„: {article_time.strftime('%Y/%m/%d %H:%M')}")
            else:
                # è¶…å‡ºæ™‚é–“çª—å£ï¼Œé–‹å§‹æ–°çš„ä¸€çµ„
                if current_group:
                    time_groups.append(current_group)
                    print(f"      ğŸ“¦ å®Œæˆçµ„åˆ¥ï¼ŒåŒ…å« {len(current_group)} ç¯‡æ–‡ç« ")
                
                # é–‹å§‹æ–°çµ„
                current_group = [item]
                current_group_start_time = article_time
                current_group_end_time = article_time + timedelta(days=time_window_days)
                print(f"      ğŸ é–‹å§‹æ–°çµ„: {current_group_start_time.strftime('%Y/%m/%d %H:%M')} - {current_group_end_time.strftime('%Y/%m/%d %H:%M')}")
    
    # æ·»åŠ æœ€å¾Œä¸€çµ„
    if current_group:
        time_groups.append(current_group)
        print(f"      ğŸ“¦ å®Œæˆæœ€å¾Œçµ„åˆ¥ï¼ŒåŒ…å« {len(current_group)} ç¯‡æ–‡ç« ")
    
    return time_groups


def save_stories_to_supabase(stories):
    """
    æ‰¹é‡ä¿å­˜æ•…äº‹å’Œæ–‡ç« åˆ°Supabaseæ•¸æ“šåº«
    """
    try:
        saved_stories = 0
        updated_stories = 0
        saved_articles = 0
        
        for story in stories:
            story_id = story["story_id"]
            action_type = story.get("action_type", "create_new_story")
            
            # æ ¹æ“š action_type æ±ºå®šå¦‚ä½•è™•ç†æ•…äº‹
            if action_type == "create_new_story":
                # ä¿å­˜æ–°æ•…äº‹
                if save_story_to_supabase(story):
                    saved_stories += 1
            elif action_type == "update_existing_story":
                # æ›´æ–°ç¾æœ‰æ•…äº‹çš„ crawl_date
                try:
                    update_data = {
                        "crawl_date": story["crawl_date"]
                    }
                    # response = supabase.table("stories").update(update_data).eq("story_id", story_id).execute()
                    print(f"   âœ… æ•…äº‹ crawl_date å·²æ›´æ–°: {story_id}")
                    updated_stories += 1
                except Exception as e:
                    print(f"   âŒ æ›´æ–°æ•…äº‹ crawl_date å¤±æ•—: {e}")
            
            # ä¿å­˜æ–‡ç« ï¼ˆç„¡è«–æ˜¯æ–°æ•…äº‹é‚„æ˜¯ç¾æœ‰æ•…äº‹ï¼‰
            for article in story["articles"]:
                if save_article_to_supabase(article, story_id):
                    saved_articles += 1
        
        print(f"âœ… æ‰¹é‡ä¿å­˜å®Œæˆ: {saved_stories} å€‹æ–°æ•…äº‹, {updated_stories} å€‹æ›´æ–°æ•…äº‹, {saved_articles} ç¯‡æ–‡ç« ")
        return True
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡ä¿å­˜åˆ°Supabaseæ™‚å‡ºéŒ¯: {e}")
        return False

def process_news_pipeline(main_url, category):
    """
    å®Œæ•´çš„æ–°èè™•ç†ç®¡é“ - æ”¹é€²çš„ WebDriver ç®¡ç†
    """
    print(f"ğŸš€ é–‹å§‹è™•ç† {category} åˆ†é¡çš„æ–°è...")
    
    # æ­¥é©Ÿ1: ç²å–æ‰€æœ‰æ•…äº‹é€£çµ
    story_links = get_main_story_links(main_url, category)
    if not story_links:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•æ•…äº‹é€£çµ")
        return []
    
    # æ­¥é©Ÿ2: è™•ç†æ¯å€‹æ•…äº‹ï¼Œç²å–æ‰€æœ‰æ–‡ç« é€£çµ
    all_article_links = []
    for story_info in story_links[:1]:
        article_links = get_article_links_from_story(story_info)
        all_article_links.extend(article_links)
    
    if not all_article_links:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ç« é€£çµ")
        return []
    
    print(f"\nğŸ“Š ç¸½å…±æ”¶é›†åˆ° {len(all_article_links)} ç¯‡æ–‡ç« å¾…è™•ç†")
    
    # æ­¥é©Ÿ3: ç²å–æ¯ç¯‡æ–‡ç« çš„å®Œæ•´å…§å®¹ - æ”¹é€²çš„éŒ¯èª¤è™•ç†
    final_articles = []
    driver = None
    consecutive_failures = 0  # é€£çºŒå¤±æ•—è¨ˆæ•¸
    max_consecutive_failures = 3  # æœ€å¤§é€£çºŒå¤±æ•—æ¬¡æ•¸
    
    def create_fresh_driver():
        """å‰µå»ºæ–°çš„ driver å¯¦ä¾‹"""
        try:
            new_driver = create_robust_driver(headless=False)
            initialize_driver_with_cookies(new_driver)
            return new_driver
        except Exception as e:
            print(f"   âŒ å‰µå»ºæ–° WebDriver å¤±æ•—: {e}")
            return None
    
    # åˆå§‹åŒ– driver
    driver = create_fresh_driver()
    if not driver:
        print("âŒ ç„¡æ³•å‰µå»ºåˆå§‹ WebDriverï¼Œçµ‚æ­¢è™•ç†")
        return []
    
    try:
        for i, article_info in enumerate(all_article_links, 1):
            print(f"\nğŸ”„ è™•ç†æ–‡ç«  {i}/{len(all_article_links)}: {article_info['article_title']}")
            
            # æª¢æŸ¥ driver æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
            try:
                # ç°¡å–®çš„ driver å¥åº·æª¢æŸ¥
                current_url = driver.current_url
            except Exception as e:
                print(f"   âš ï¸ WebDriver ç•°å¸¸ï¼Œé‡æ–°å‰µå»º: {e}")
                try:
                    driver.quit()
                except:
                    pass
                driver = create_fresh_driver()
                if not driver:
                    print(f"   âŒ ç„¡æ³•é‡æ–°å‰µå»º WebDriverï¼Œè·³éå‰©é¤˜ {len(all_article_links) - i + 1} ç¯‡æ–‡ç« ")
                    break
            
            article_content = get_final_content(article_info, driver)
            
            if article_content:
                final_articles.append(article_content)
                print(f"   âœ… æˆåŠŸç²å–å…§å®¹")
                consecutive_failures = 0  # é‡ç½®é€£çºŒå¤±æ•—è¨ˆæ•¸
                
            else:
                print(f"   âŒ ç„¡æ³•ç²å–å…§å®¹")
                consecutive_failures += 1
                
                # æª¢æŸ¥æ˜¯å¦éœ€è¦é‡æ–°å‰µå»º driver
                if consecutive_failures >= max_consecutive_failures:
                    print(f"   ğŸ”„ é€£çºŒ {consecutive_failures} æ¬¡å¤±æ•—ï¼Œé‡æ–°å‰µå»º WebDriver...")
                    
                    try:
                        driver.quit()
                    except:
                        pass
                    
                    driver = create_fresh_driver()
                    if not driver:
                        print(f"   âŒ ç„¡æ³•é‡æ–°å‰µå»º WebDriverï¼Œè·³éå‰©é¤˜ {len(all_article_links) - i + 1} ç¯‡æ–‡ç« ")
                        break
                    
                    consecutive_failures = 0  # é‡ç½®è¨ˆæ•¸
                    print(f"   âœ… WebDriver é‡æ–°å‰µå»ºå®Œæˆ")
                    
                    # å¯é¸ï¼šé‡æ–°å˜—è©¦ç•¶å‰æ–‡ç« 
                    print(f"   ğŸ”„ é‡æ–°å˜—è©¦è™•ç†ç•¶å‰æ–‡ç« ...")
                    article_content = get_final_content(article_info, driver)
                    if article_content:
                        final_articles.append(article_content)
                        print(f"   âœ… é‡æ–°å˜—è©¦æˆåŠŸ")
                    else:
                        print(f"   âŒ é‡æ–°å˜—è©¦ä»ç„¶å¤±æ•—")
            
            # éš¨æ©Ÿå»¶é²
            time.sleep(random.randint(2, 4))
            
    except KeyboardInterrupt:
        print(f"\nâš¡ ç”¨æˆ¶ä¸­æ–·è™•ç†")
        
    except Exception as e:
        print(f"\nğŸ’¥ è™•ç†éç¨‹ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
        import traceback
        print(f"ğŸ“‹ éŒ¯èª¤è©³æƒ…:\n{traceback.format_exc()}")
        
    finally:
        if driver:
            try:
                print(f"\nğŸ”§ æ¸…ç† WebDriver è³‡æº...")
                driver.quit()
                print(f"   âœ… WebDriver æ¸…ç†å®Œæˆ")
            except Exception as e:
                print(f"   âš ï¸ WebDriver æ¸…ç†æ™‚å‡ºç¾å•é¡Œ: {e}")
    
    print(f"\nğŸ“Š æ–‡ç« å…§å®¹ç²å–å®Œæˆ: æˆåŠŸ {len(final_articles)}/{len(all_article_links)} ç¯‡")
    
    # æ­¥é©Ÿ4: æŒ‰æ•…äº‹å’Œæ™‚é–“åˆ†çµ„
    final_stories = group_articles_by_story_and_time(final_articles, time_window_days=3)
    
    return final_stories

def initialize_driver_with_cookies(driver):
    """åˆå§‹åŒ– WebDriver ä¸¦è¼‰å…¥ cookies"""
    try:
        # å…ˆè¨ªå• Google News ä¸»é 
        driver.get("https://news.google.com/")
        time.sleep(2)
        
        # å˜—è©¦è¼‰å…¥ cookies
        try:
            with open("cookies.json", "r", encoding="utf-8") as f:
                cookies = json.load(f)
            
            for cookie in cookies:
                if 'sameSite' in cookie:
                    cookie.pop('sameSite')
                try:
                    driver.add_cookie(cookie)
                except Exception as e:
                    print(f"âš ï¸ ç„¡æ³•æ·»åŠ  cookie: {e}")
            
            print("âœ… Cookies è¼‰å…¥å®Œæˆ")
            
        except FileNotFoundError:
            print("âš ï¸ cookies.json æª”æ¡ˆä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜èªè¨­ç½®")
    
    except Exception as e:
        print(f"âš ï¸ åˆå§‹åŒ– WebDriver cookies æ™‚å‡ºéŒ¯: {e}")

def main():
    """
    ä¸»å‡½æ•¸ - æ–°èçˆ¬èŸ²çš„å…¥å£é»
    """
    print("="*80)
    print("ğŸŒŸ Google News çˆ¬èŸ²ç¨‹åºå•Ÿå‹•")
    print("="*80)

    # é…ç½®éœ€è¦è™•ç†çš„æ–°èåˆ†é¡
    news_categories = {
        "Politics": "https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNRFZ4ZERBU0JYcG9MVlJYS0FBUAE?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        "Taiwan News": "https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNRFptTXpJU0JYcG9MVlJYS0FBUAE?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        "International News": "https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRGx1YlY4U0JYcG9MVlJYR2dKVVZ5Z0FQAQ?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        "Science & Technology": "https://news.google.com/topics/CAAqLAgKIiZDQkFTRmdvSkwyMHZNR1ptZHpWbUVnVjZhQzFVVnhvQ1ZGY29BQVAB?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        "Lifestyle & Consumer": "https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSkwyMHZNREUwWkhONEVnVjZhQzFVVnlnQVAB?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        "Sports": "https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRFp1ZEdvU0JYcG9MVlJYR2dKVVZ5Z0FQAQ?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        "Entertainment": "https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNREpxYW5RU0JYcG9MVlJYR2dKVVZ5Z0FQAQ?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        "Business & Finance": "https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRGx6TVdZU0JYcG9MVlJYR2dKVVZ5Z0FQAQ?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        "Health & Wellness": "https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNR3QwTlRFU0JYcG9MVlJYS0FBUAE?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant"
    }

    
    # å¯ä»¥é¸æ“‡è™•ç†ç‰¹å®šåˆ†é¡æˆ–å…¨éƒ¨åˆ†é¡
    # selected_categories = ["Science & Technology"]#, "Business & Finance", "Health & Wellness", "Sports", "Entertainment", "Lifestyle & Consumer", ]#"Taiwan News", "International News", "Politics"]# å¯ä»¥ä¿®æ”¹é€™è£¡ä¾†é¸æ“‡è¦è™•ç†çš„åˆ†é¡
    selected_categories = ["Politics"]
    # selected_categories = list(news_categories.keys())  # è™•ç†æ‰€æœ‰åˆ†é¡
    
    all_final_stories = []
    start_time = time.time()
    
    try:
        for category in selected_categories:
            if category not in news_categories:
                print(f"âš ï¸ æœªçŸ¥çš„åˆ†é¡: {category}")
                continue
                
            category_start_time = time.time()
            print(f"\n{'='*60}")
            print(f"ğŸ¯ é–‹å§‹è™•ç†åˆ†é¡: {category}")
            print(f"{'='*60}")
            
            # è™•ç†è©²åˆ†é¡çš„æ–°è
            category_stories = process_news_pipeline(news_categories[category], category)
            
            if category_stories:
                all_final_stories.extend(category_stories)
                category_end_time = time.time()
                category_duration = category_end_time - category_start_time
                
                print(f"\nâœ… {category} åˆ†é¡è™•ç†å®Œæˆ!")
                print(f"   ğŸ“Š ç²å¾— {len(category_stories)} å€‹æ•…äº‹")
                print(f"   â±ï¸  è€—æ™‚: {category_duration:.2f} ç§’")
            else:
                print(f"\nâŒ {category} åˆ†é¡è™•ç†å¤±æ•—ï¼Œæ²’æœ‰ç²å¾—ä»»ä½•æ•…äº‹")
            
            # åˆ†é¡ä¹‹é–“çš„å»¶é²
            if category != selected_categories[-1]:  # ä¸æ˜¯æœ€å¾Œä¸€å€‹åˆ†é¡
                print(f"\nâ³ ç­‰å¾… 30 ç§’å¾Œè™•ç†ä¸‹ä¸€å€‹åˆ†é¡...")
                time.sleep(30)
        
        # è™•ç†å®Œæˆå¾Œçš„çµ±è¨ˆ
        total_end_time = time.time()
        total_duration = total_end_time - start_time
        
        print(f"\n{'='*80}")
        print(f"ğŸ‰ æ‰€æœ‰åˆ†é¡è™•ç†å®Œæˆ!")
        print(f"{'='*80}")
        print(f"ğŸ“Š æœ€çµ‚çµ±è¨ˆ:")
        print(f"   ğŸ·ï¸  è™•ç†åˆ†é¡æ•¸: {len(selected_categories)}")
        print(f"   ğŸ“° ç¸½æ•…äº‹æ•¸: {len(all_final_stories)}")
        
        # çµ±è¨ˆæ¯å€‹åˆ†é¡çš„æ•…äº‹æ•¸
        category_counts = {}
        total_articles = 0
        for story in all_final_stories:
            category = story['category']
            category_counts[category] = category_counts.get(category, 0) + 1
            total_articles += len(story['articles'])
        
        for category, count in category_counts.items():
            print(f"   ğŸ“‚ {category}: {count} å€‹æ•…äº‹")
        
        print(f"   ğŸ“„ ç¸½æ–‡ç« æ•¸: {total_articles}")
        print(f"   â±ï¸  ç¸½è€—æ™‚: {total_duration:.2f} ç§’ ({total_duration/60:.1f} åˆ†é˜)")
        
        # ä¿å­˜æ•¸æ“š
        if all_final_stories:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            all_final_stories = clean_data(all_final_stories)
            
            # ä¿å­˜åˆ°æ•¸æ“šåº«ï¼ˆå¦‚æœéœ€è¦ï¼‰
            try:
                save_stories_to_supabase(all_final_stories)
                print("ğŸ’¾ æ•¸æ“šåº«ä¿å­˜: å·²è·³é (è«‹æ ¹æ“šéœ€è¦å¯¦ç¾)")
            except Exception as e:
                print(f"âŒ æ•¸æ“šåº«ä¿å­˜å¤±æ•—: {e}")
            
        else:
            print("âš ï¸ æ²’æœ‰ç²å¾—ä»»ä½•æ•…äº‹æ•¸æ“š")
    
    except KeyboardInterrupt:
        print(f"\nâš¡ ç¨‹åºè¢«ç”¨æˆ¶ä¸­æ–·")
        if all_final_stories:
            # å³ä½¿è¢«ä¸­æ–·ï¼Œä¹Ÿä¿å­˜å·²ç²å–çš„æ•¸æ“š
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    except Exception as e:
        print(f"\nğŸ’¥ ç¨‹åºåŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        print(f"ğŸ“‹ éŒ¯èª¤è©³æƒ…:\n{traceback.format_exc()}")
    
    finally:
        print(f"\n{'='*80}")
        print(f"ğŸ‘‹ Google News çˆ¬èŸ²ç¨‹åºçµæŸ")
        print(f"{'='*80}")

if __name__ == "__main__":
    main()